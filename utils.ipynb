{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1d2Yf6dyRwiKm_K193rmi2lXvrRM4M02X",
      "authorship_tag": "ABX9TyPOSq3JQ6f/9oqFe+rDj4Ie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimikuo365/Bike-Rider-Facilitating-System/blob/master/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from skimage import io\n",
        "import tifffile\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "yem4zcxszJX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the needed index based on the wanted bands\n",
        "def get_wanted_image_band(selected_band, img):\n",
        "  selected_img = np.array(img[:,:,selected_band])\n",
        "  selected_img = np.reshape(selected_img, (selected_img.shape[0], selected_img.shape[1], 1))\n",
        "  return selected_img"
      ],
      "metadata": {
        "id": "eG5nGip7-7eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Circle the rectangle of the corresponding tile\n",
        "def draw_rect(ax, xy, tile_height, tile_width):\n",
        "  rect = patches.Rectangle(xy, tile_width, tile_height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "  ax.add_patch(rect)"
      ],
      "metadata": {
        "id": "yHrnaHkM-5sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fontsize(tiles_in_band):\n",
        "  if tiles_in_band <= 16: \n",
        "    return 7\n",
        "  elif tiles_in_band <= 64:\n",
        "    return 5\n",
        "  elif tiles_in_band <= 256:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "BLfxoMaB-4uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_text(ax, color, setting, values):\n",
        "  rx, ry = values['coordinate']\n",
        "  tile_height, tile_width = setting['tile_height'], setting['tile_width']\n",
        "\n",
        "  cx = rx + tile_width / 2.0\n",
        "  cy = ry + tile_height / 2.0\n",
        "\n",
        "  diff = values['current'] - values['average']\n",
        "  cur_value = str(int(values['current'])) \n",
        "  avg_value = str(int(values['average'])) \n",
        "  threshold = str(int(values['threshold']))\n",
        "\n",
        "  if diff != 0:\n",
        "    txt = cur_value + '-' + avg_value\n",
        "    txt += '\\n(' + threshold + ')'\n",
        "  else:\n",
        "    txt = avg_value\n",
        "  \n",
        "  fontsize = get_fontsize(setting['height_split'] * setting['width_split'])\n",
        "  ax.annotate(txt, (cx, cy), fontsize=fontsize, color=color, ha='center', va='center')"
      ],
      "metadata": {
        "id": "UKsCvG_z-3Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_annotation(is_swapped, ax, setting, values):\n",
        "  color = 'y'\n",
        "  if is_swapped:\n",
        "    color = 'r'\n",
        "    draw_rect(ax, values['coordinate'], setting['tile_height'], setting['tile_width'])\n",
        "  add_text(ax, color, setting, values)"
      ],
      "metadata": {
        "id": "1siel3fy-1li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pta_-reGy7rk"
      },
      "outputs": [],
      "source": [
        "def check_folder(path):\n",
        "  # Check whether the specified path exists or not\n",
        "  isExist = os.path.exists(path)\n",
        "  # Create a new directory if not exist \n",
        "  if not isExist:\n",
        "    os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_threshold(ratio_ls, origin_threshold_ls):\n",
        "  updated_threshold_ls = []\n",
        "  for ratio, orign_threshold in zip(ratio_ls, origin_threshold_ls):\n",
        "    updated_threshold = ratio * orign_threshold\n",
        "    updated_threshold_ls.append(updated_threshold)\n",
        "  return updated_threshold_ls"
      ],
      "metadata": {
        "id": "mWOwKSqg5n27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_cases(setting, case_ls):\n",
        "  if not setting['perform_smooth']:\n",
        "    return case_ls\n",
        "\n",
        "  new_case_ls = []\n",
        "  train_num = int(len(case_ls) * setting['train_val_ratio']) + setting['lstm_weeks']\n",
        "  half_smoothing_length = int(setting['smoothing_length'] / 2)\n",
        "\n",
        "  for i, case in enumerate(case_ls[:train_num]):\n",
        "    start_idx, end_idx = 0, (train_num - 1)\n",
        "    if i >= half_smoothing_length:\n",
        "      start_idx = i - half_smoothing_length\n",
        "    if i + half_smoothing_length < train_num:\n",
        "      end_idx = i + half_smoothing_length\n",
        "    new_case = np.average(case_ls[start_idx : end_idx + 1])\n",
        "    new_case_ls.append(new_case)\n",
        "  \n",
        "  # append the remaining testing set's value without smoothing\n",
        "  new_case_ls.extend(case_ls[train_num:])   \n",
        "  \n",
        "  return np.array(new_case_ls)"
      ],
      "metadata": {
        "id": "MCU-Ov3Y_KGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlation(prediction, real):\n",
        "  prediction = prediction.flatten()\n",
        "  real = real.flatten()\n",
        "  corr = np.corrcoef(prediction, real)\n",
        "  print(prediction)\n",
        "  print(real)\n",
        "  return corr[0][1]"
      ],
      "metadata": {
        "id": "1G_e5wc7_M2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_group_value(tile_ls, setting):\n",
        "  group_ls = []\n",
        "  for row in range(0, setting['height_split'], setting['height_group']):\n",
        "    for col in range(0, setting['width_split'], setting['width_group']):\n",
        "      index_in_group = [] # for debug purpose\n",
        "      tiles_in_group = []\n",
        "      for r in range(row, row + setting['height_group']):\n",
        "        for c in range(col, col + setting['width_group']):\n",
        "          index = r * setting['width_split'] + c\n",
        "          index_in_group.append(index)\n",
        "          tiles_in_group.append(tile_ls[index])\n",
        "      cur_group_value = np.average(tiles_in_group)\n",
        "      group_ls.append(cur_group_value)\n",
        "  return group_ls"
      ],
      "metadata": {
        "id": "yGDvYdnw_Irf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import xor\n",
        "def split_train_test(setting, input, output):\n",
        "  train_num = int(len(output) * setting['train_val_ratio']) + setting['lstm_weeks']\n",
        "  lstm_weeks = setting['lstm_weeks']\n",
        "  \n",
        "  scaler = MinMaxScaler(feature_range=(0, 1)) \n",
        "  input = np.array(input)\n",
        "  X = scaler.fit_transform(input)\n",
        "\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1)) \n",
        "  y = scaler.fit_transform(output)\n",
        "\n",
        "  X_train = X[:train_num,:]\n",
        "  X_test = X[train_num - lstm_weeks:,:]\n",
        "  y_train = y[:train_num]\n",
        "  y_test = y[train_num - lstm_weeks:]\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, scaler"
      ],
      "metadata": {
        "id": "Nd7q7zSF_HuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_img(data_dic, train_num, save_path):\n",
        "  counter = 0\n",
        "  all_img = None\n",
        "  img_ls = data_dic['image'][:train_num]\n",
        "  class_ls = data_dic['class'][:train_num]\n",
        "  name_ls = data_dic['epiweek'][:train_num]\n",
        "\n",
        "  for img, category, name in zip(img_ls, class_ls, name_ls):\n",
        "    if category != 'good':\n",
        "      continue\n",
        "    print(name)\n",
        "    all_img = img.astype('float64') if all_img is None else (all_img + img.astype('float64'))\n",
        "    counter += 1\n",
        "\n",
        "  # Read the training set's average image \n",
        "  all_img = all_img.astype('float64')\n",
        "  avg_img = all_img / counter\n",
        "  avg_img = avg_img.astype('uint8')\n",
        "\n",
        "  plt.imshow(avg_img, cmap='gray')\n",
        "  plt.savefig(save_path, dpi=300)\n",
        "  print(avg_img.shape)\n",
        "\n",
        "  return avg_img"
      ],
      "metadata": {
        "id": "hcvst_sN_G50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_row_col_pair(cur_img, setting):\n",
        "  pair_ls = []\n",
        "  height, width = cur_img.shape\n",
        "  tile_height, tile_width = setting['tile_height'], setting['tile_width']\n",
        "\n",
        "  for r in range(0, height - tile_height + 1, tile_height):\n",
        "    for c in range(0, width - tile_width + 1, tile_width):\n",
        "        pair_ls.append([c, r])\n",
        "  return np.array(pair_ls)"
      ],
      "metadata": {
        "id": "qKb-NfUw_GIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split images into tiles by indicating the window size for rows and columns\n",
        "def split_img_to_tiles(cur_img, setting):\n",
        "  tile_ls = []\n",
        "  height, width = cur_img.shape\n",
        "  tile_height, tile_width = setting['tile_height'], setting['tile_width']\n",
        "\n",
        "  for r in range(0, height - tile_height + 1, tile_height):\n",
        "    for c in range(0, width - tile_width + 1, tile_width):\n",
        "      tile = cur_img[r : r + tile_height, c : c + tile_width]\n",
        "      tile_avg = round(np.average(tile), 3)\n",
        "      tile_ls.append(tile_avg)\n",
        "\n",
        "  return np.array(tile_ls)"
      ],
      "metadata": {
        "id": "3Rok5hhf_FRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(option, title, max_y, min_y, y_train_pred, y_train, y_test_pred, y_test):\n",
        "  plt.ylim(min_y, max_y)\n",
        "  if option == \"combine\":\n",
        "    y_pred = np.concatenate((y_train_pred, y_test_pred), axis=None)\n",
        "    y_true = np.concatenate((y_train, y_test), axis=None)\n",
        "    plt.plot(y_pred, label = 'predicted', color='c')\n",
        "    plt.plot(y_true, label = 'actual', color='b')\n",
        "    plt.plot([len(y_train), len(y_train)], [min_y, max_y], color=\"r\")\n",
        "\n",
        "  else:  \n",
        "    train_ls = [*range(0, len(y_train), 1)]\n",
        "    test_ls = [*range(len(y_train), len(y_train) + len(y_test), 1)]\n",
        "\n",
        "    plt.plot(train_ls, y_train_pred, label = 'train predicted', color=(1, 0, 0, 0.5))\n",
        "    plt.plot(train_ls, y_train, label = 'train actual', color=(0, 1, 0, 0.5))\n",
        "    plt.plot(test_ls, y_test_pred, label = 'test predicted', color=(1, 0, 0, 1))\n",
        "    plt.plot(test_ls, y_test, label = 'test actual', color=(0, 1, 0, 1))\n",
        "\n",
        "  plt.legend(loc=\"upper left\")\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RdPlbFgm_EB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import  mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def prepare_time_series(data, n_in=1, n_out=1, dropnan=True):\n",
        "  n_vars = 1 if type(data) is list else data.shape[1]\n",
        "  df = pd.DataFrame(data)\n",
        "  cols, names = list(), list()\n",
        "\n",
        "  # input sequence (t-n, ... t-1)\n",
        "  for i in range(n_in, 0, -1):\n",
        "    cols.append(df.shift(i))\n",
        "    names += [(f'var{j+1}(t-{i})') for j in range(n_vars)]\n",
        "\n",
        "  # forecast sequence (t, t+1, ... t+n)\n",
        "  for i in range(0, n_out):\n",
        "    cols.append(df.shift(-i))\n",
        "    if i == 0:\n",
        "      names += [(f'var({j+1})') for j in range(n_vars)]\n",
        "    else:\n",
        "      names += [(f'var{j+1}(t+{i})') for j in range(n_vars)]\n",
        "\n",
        "  agg = pd.concat(cols, axis=1)\n",
        "  agg.columns = names\n",
        "\n",
        "  # drop rows with NaN values\n",
        "  if dropnan:\n",
        "    agg.dropna(inplace=True)\n",
        "\n",
        "  return np.array(agg)\n",
        "\n",
        "def removeLastWeek(data, n_features):\n",
        "  return data[:,:-n_features]"
      ],
      "metadata": {
        "id": "WFQA4sZq_C_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tile_transformation_on_all(data_dic, setting):\n",
        "  # Perform image to tile transformation for every image \n",
        "  tile_ls = []\n",
        "  for img in data_dic['image']:\n",
        "    tiles = split_img_to_tiles(img, setting)\n",
        "    tile_ls.append(tiles)\n",
        "  return np.array(tile_ls)"
      ],
      "metadata": {
        "id": "1o5M4WzI_B2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pixels_per_tile(img, height_split, width_split):\n",
        "  height, width = img.shape\n",
        "  tile_height = int(height / height_split)\n",
        "  tile_width = int(width / width_split)\n",
        "  return tile_height, tile_width"
      ],
      "metadata": {
        "id": "fF3q3hp7_A4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_to_dict(filename):\n",
        "  df = pd.read_csv(filename)\n",
        "  # Create a dictionary to store wanted columns from CSV\n",
        "  dic = {\n",
        "      'epiweek': [],\n",
        "      'image': [],\n",
        "      'cases': [],\n",
        "      'class': []\n",
        "      }\n",
        "  \n",
        "  for index, row in df.iterrows():\n",
        "    img_loc = row['img_loc']\n",
        "    img = io.imread(img_loc)\n",
        "\n",
        "    dic['image'].append(img)\n",
        "    dic['epiweek'].append(row['epiweek'])\n",
        "    dic['cases'].append(row['Cases'])\n",
        "    dic['class'].append(row['class'])\n",
        "\n",
        "  dic['epiweek'] = np.array(dic['epiweek'])\n",
        "  dic['image'] = np.array(dic['image'])\n",
        "  dic['cases'] = np.array(dic['cases'])\n",
        "  dic['class'] = np.array(dic['class'])\n",
        "\n",
        "  return dic"
      ],
      "metadata": {
        "id": "K0TyLz0U-_yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(X_train, X_test, y_train, y_test, scaler):\n",
        "  tf.random.set_seed(1)\n",
        "  callback = X_train.shape[1]\n",
        "  features_num = X_train.shape[2]\n",
        "\n",
        "  # design network\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(60, input_shape=(callback, features_num), return_sequences=True))\n",
        "  model.add(LSTM(30))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "  history = model.fit(X_train, y_train, epochs=100, verbose=0, batch_size=72, \n",
        "                      validation_data=(X_test, y_test))\n",
        "  \n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.title('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  y_train = scaler.inverse_transform(y_train)\n",
        "  y_test = scaler.inverse_transform(y_test)\n",
        "  \n",
        "  max_y = max(max(y_train), max(y_test)) + 5\n",
        "  min_y = min(min(y_train), min(y_test)) - 5\n",
        "\n",
        "  # make a prediction\n",
        "  y_pred = model.predict(X_train)\n",
        "  y_train_pred = scaler.inverse_transform(y_pred)\n",
        "  \n",
        "  y_pred = model.predict(X_test)\n",
        "  y_test_pred = scaler.inverse_transform(y_pred)\n",
        "\n",
        "  title = 'Prediction'\n",
        "  make_prediction(\"combine\", title, max_y, min_y, y_train_pred, y_train, y_test_pred, y_test)\n",
        "  \n",
        "  \n",
        "  mae = mean_absolute_error(y_test, y_test_pred)\n",
        "  corr = get_correlation(y_test_pred, y_test)\n",
        "  print('MAE: %.2f' % mae)\n",
        "  print('Correlation: %.2f' % corr)"
      ],
      "metadata": {
        "id": "u4s_UN-M_OxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tile_swapping(tile_info, img_info=None, setting=None):\n",
        "  updated_tiles = tile_info['current'].copy()\n",
        "  swapped_counter = 0\n",
        "  swapped_index = []\n",
        "  tile_num = len(tile_info['threshold'])\n",
        "  \n",
        "  for i in range(tile_num):\n",
        "    threshold = tile_info['threshold'][i]\n",
        "    cur_tile_value = tile_info['current'][i]\n",
        "    avg_tile_value = tile_info['average'][i]\n",
        "    diff = np.absolute(cur_tile_value - avg_tile_value)\n",
        "    \n",
        "    if diff > threshold:\n",
        "      swapped_index.append(i)\n",
        "      updated_tiles[i] = avg_tile_value\n",
        "      swapped_counter += 1\n",
        "\n",
        "  ratio = swapped_counter / tile_num\n",
        "  if img_info != None:\n",
        "    select_mask_region(img_info, tile_info, swapped_index, setting)\n",
        "  \n",
        "  return updated_tiles, ratio"
      ],
      "metadata": {
        "id": "IuzqityG_P23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tile_swapping_and_grouping(tile_ls, data_dic, threshold_ls, avg_img_tiles, setting):\n",
        "  preprocessed_tile_ls, swapped_ratio_ls = [], []\n",
        "  data_dic['swap ratio'] = []\n",
        "  data_dic['predicted_class'] = []\n",
        "  for i, cur_img_tiles in enumerate(tile_ls):\n",
        "    img = data_dic['image'][i]\n",
        "    epiweek = data_dic['epiweek'][i]\n",
        "\n",
        "    print(epiweek)\n",
        "    tile_info = {\n",
        "        'threshold': threshold_ls,\n",
        "        'average': avg_img_tiles,\n",
        "        'current': cur_img_tiles\n",
        "    }\n",
        "\n",
        "    img_info = {\n",
        "        'image': img,\n",
        "        'epiweek': str(epiweek),\n",
        "        'path': setting['tile_swap_result_folder']\n",
        "    }\n",
        "\n",
        "    # swapping\n",
        "    if setting['run_all_img_and_save_result']:\n",
        "      updated_img_tiles, ratio = tile_swapping(tile_info, img_info, setting)\n",
        "    else:\n",
        "      updated_img_tiles, ratio = tile_swapping(tile_info)\n",
        "\n",
        "    # save prediction results\n",
        "    data_dic['swap ratio'].append(ratio)\n",
        "    if ratio > 0.5:\n",
        "      data_dic['predicted_class'].append('bad')\n",
        "    elif ratio > 0.1:\n",
        "      data_dic['predicted_class'].append('normal')\n",
        "    else:\n",
        "      data_dic['predicted_class'].append('good')\n",
        "\n",
        "    # grouping\n",
        "    group_avg = calculate_group_value(updated_img_tiles, setting)\n",
        "    preprocessed_tile_ls.append(group_avg)\n",
        "    swapped_ratio_ls.append(ratio)\n",
        "\n",
        "  print('Swap ratio:', round(np.average(swapped_ratio_ls), 3))\n",
        "  return np.array(preprocessed_tile_ls)"
      ],
      "metadata": {
        "id": "hMjOpIBc_RIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_mask_region(img_info, tile_info, swapped_index, setting):\n",
        "  fig, ax = plt.subplots(constrained_layout=True)\n",
        "  ax.axis('off')\n",
        "  plt.imshow(img_info['image'], cmap='gray')\n",
        "  tile_num = setting['height_split'] * setting['width_split']\n",
        "\n",
        "  for i in range(tile_num):\n",
        "    values = {\n",
        "      'coordinate': setting['row_col_pairs'][i],\n",
        "      'current': tile_info['current'][i],\n",
        "      'average': tile_info['average'][i],\n",
        "      'threshold': tile_info['threshold'][i]\n",
        "    }\n",
        "\n",
        "    if i in swapped_index:\n",
        "      add_annotation(True, ax, setting, values)\n",
        "    else:\n",
        "      add_annotation(False, ax, setting, values)\n",
        "\n",
        "  plt.title(img_info['epiweek'] + ' (' + str(len(swapped_index)) + ')')\n",
        "  location = os.path.join(img_info['path'], img_info['epiweek'] + '.png')\n",
        "  print(location)\n",
        "  plt.savefig(location, dpi=300)\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "TeBv-eP8_SAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_XY_for_lstm(preprocessed_tile_ls, case_ls, setting):\n",
        "  # print(case_ls.shape) # (156,)\n",
        "  reshaped_case_ls = case_ls.reshape(-1, 1)\n",
        "  # print(reshaped_case_ls.shape) #(156, 1)\n",
        "\n",
        "  X_train, X_test, y_train, y_test, scaler = split_train_test(setting, preprocessed_tile_ls, reshaped_case_ls)\n",
        "  # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) # (134, 256) (32, 256) (134, 1) (32, 1)\n",
        "\n",
        "  X_train_series = prepare_time_series(X_train, n_in=setting['lstm_weeks'])\n",
        "  X_test_series = prepare_time_series(X_test, n_in=setting['lstm_weeks'])\n",
        "  # print(X_train_series.shape, X_test_series.shape) # (124, 2816) (22, 2816)\n",
        "\n",
        "  n_features = X_train.shape[1]\n",
        "  X_train_series_clean = removeLastWeek(X_train_series, n_features)\n",
        "  X_test_series_clean = removeLastWeek(X_test_series, n_features)\n",
        "  # print(X_train_series_clean.shape, X_test_series_clean.shape) # (124, 2560) (22, 2560)\n",
        "\n",
        "  final_X_train = X_train_series_clean.reshape((-1, setting['lstm_weeks'], n_features))\n",
        "  final_X_test = X_test_series_clean.reshape((-1, setting['lstm_weeks'], n_features))\n",
        "  # print(final_X_train.shape, final_X_test.shape) # (124, 10, 256) (22, 10, 256)\n",
        "\n",
        "  final_y_train = y_train[setting['lstm_weeks']:]\n",
        "  final_y_test = y_test[setting['lstm_weeks']:]\n",
        "  # print('Final output:', final_y_train.shape, final_y_test.shape) # (124, 1) (22, 1)\n",
        "\n",
        "  return final_X_train, final_X_test, final_y_train, final_y_test, scaler"
      ],
      "metadata": {
        "id": "ID5OFeDO_TGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}